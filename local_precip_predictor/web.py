
from datetime import datetime
import requests
import requests_cache
from retry_requests import retry
from urllib.request import urlopen

from bs4 import BeautifulSoup, Comment
import openmeteo_requests
import pandas as pd


class State:

    def __init__(
            self,
            dataframe=None,
            exported_to_file=False,
            filename=None,
    ):
        self.df = dataframe
        self._exported_to_file = exported_to_file
        self._filename = filename if exported_to_file else None
    
    @property
    def filename(self):
        return self._filename
    
    @property
    def exported_to_file(self):
        return self._exported_to_file

    @filename.setter
    def filename(self, filename):
        self._exported_to_file = filename is not None
        self._filename = filename


def get_nao_data(
    export_to_file=False
):
    '''
    Get the North Atlantic Oscillation values for each month of the year. The 
    data starts in 1950. 

    :param export_to_file: When true, the dataframe will be saved to a file nao.csv.
    :return: State object
    '''
    # Get the data from the NOAA ascii table. This table is updated monthly.
    url = "https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii.table"
    html = requests.get(url).content
    # format the code as a string rather than bytes
    data_str = html.decode('utf-8')
    # Split the data by lines in the table. The first row will be the
    # header (the months of the year). The rest of the lines are the rows
    # in the table, but the first element is the year. Cut the year out
    # and set that as the row index for the dataframe.
    lines = data_str.split("\n")
    head, lines = lines[0].split(), [x.split() for x in lines[1:] if x]
    row_index = [x[0] for x in lines if x]
    lines = [x[1:] for x in lines]
    df = pd.DataFrame(lines, columns=head)
    df.index = row_index

    if export_to_file:
        df.to_csv("nao.csv")

    state = State(
        dataframe=df,
        exported_to_file=export_to_file,
        filename="nao.csv"
    )
    return state


def get_daily_data(
        latitude=36.8506,
        longitude=-75.9779,
        timezone="America/New_York",
        start_date="1950-01-01",
        end_date="1950-01-01",
        export_to_file=False
):
    '''
    Get the daily data from the api. The information included in the output:
    - temperature_2m_max
    - temperature_2m_min
    - temperature_2m_mean
    - apparent_temperature_max
    - apparent_temperature_min
    - apparent_temperature_mean
    - precipitation_sum
    - rain_sum
    - snowfall_sum

    The following units are used:
    - temperature_unit: fahrenheit
    - wind_speed_unit: mph
    - precipitation_unit: inch
    
    :param latitude: Latitude where measurements take place. [default = Latitude of Virginia Beach]
    :param longitude: Longitude where measurements take place. [default = Longitude of Virginia Beach]
    :param timezone: Timezone where the measurements take place.
    :param start_date: yyyy-mm-dd Formatted date to begin measurements.
    :param end_date: yyyy-mm-dd Formatted date to end measurements
    :param export_to_file: When true, export to a csv file with the dates as the name.

    ** Disclaimer: The majority of this function is generated by open meteo through their website. **

    :return: State object
    '''
    # Setup the Open-Meteo API client with cache and retry on error
    cache_session = requests_cache.CachedSession('.cache', expire_after = -1)
    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)
    openmeteo = openmeteo_requests.Client(session = retry_session)

    # Make sure all required weather variables are listed here
    # The order of variables in hourly or daily is important to assign them correctly below
    url = "https://archive-api.open-meteo.com/v1/archive"
    params = {
	"latitude": latitude,
	"longitude": longitude,
	"start_date": start_date,
	"end_date": end_date,
	"daily": [
            "temperature_2m_max",
            "temperature_2m_min",
            "temperature_2m_mean",
            "apparent_temperature_max",
            "apparent_temperature_min",
            "apparent_temperature_mean",
            "precipitation_sum",
            "rain_sum",
            "snowfall_sum"
        ],
	"temperature_unit": "fahrenheit",
	"wind_speed_unit": "mph",
	"precipitation_unit": "inch",
	"timezone": timezone
    }
    responses = openmeteo.weather_api(url, params=params)

    # Process first location. Add a for-loop for multiple locations or weather models
    response = responses[0]

    # Process daily data. The order of variables needs to be the same as requested.
    daily = response.Daily()
    daily_temperature_2m_max = daily.Variables(0).ValuesAsNumpy()
    daily_temperature_2m_min = daily.Variables(1).ValuesAsNumpy()
    daily_temperature_2m_mean = daily.Variables(2).ValuesAsNumpy()
    daily_apparent_temperature_max = daily.Variables(3).ValuesAsNumpy()
    daily_apparent_temperature_min = daily.Variables(4).ValuesAsNumpy()
    daily_apparent_temperature_mean = daily.Variables(5).ValuesAsNumpy()
    daily_precipitation_sum = daily.Variables(6).ValuesAsNumpy()
    daily_rain_sum = daily.Variables(7).ValuesAsNumpy()
    daily_snowfall_sum = daily.Variables(8).ValuesAsNumpy()
    
    daily_data = {
        "date": pd.date_range(
            start = pd.to_datetime(daily.Time(), unit = "s", utc = True),
            end = pd.to_datetime(daily.TimeEnd(), unit = "s", utc = True),
            freq = pd.Timedelta(seconds = daily.Interval()),
            inclusive = "left"
        )
    }
    daily_data["temperature_2m_max"] = daily_temperature_2m_max
    daily_data["temperature_2m_min"] = daily_temperature_2m_min
    daily_data["temperature_2m_mean"] = daily_temperature_2m_mean
    daily_data["apparent_temperature_max"] = daily_apparent_temperature_max
    daily_data["apparent_temperature_min"] = daily_apparent_temperature_min
    daily_data["apparent_temperature_mean"] = daily_apparent_temperature_mean
    daily_data["precipitation_sum"] = daily_precipitation_sum
    daily_data["rain_sum"] = daily_rain_sum
    daily_data["snowfall_sum"] = daily_snowfall_sum

    daily_dataframe = pd.DataFrame(data=daily_data)
    if export_to_file:
        daily_dataframe.to_csv(f'open_meteo_{start_date}_{end_date}.csv', index=False)

    state = State(
        dataframe=daily_dataframe,
        exported_to_file=export_to_file,
        filename=f'open_meteo_{start_date}_{end_date}.csv'
    )
    return state


def _is_comment(elem):
    return isinstance(elem, Comment)


def get_enso_data(export_to_file=False):
    '''
    Pull the ENSO data from NOAA's Climate Prediction Center. 

    Negative values indicate La Nina (cold)
    Positive values indicate El Nino (warm)

    The break down of values is categorized further by the following criteria:
    Weak: 0.5 - 0.9
    Moderate: 1.0 - 1.4
    Strong: 1.5 - 1.9
    Very Strong: >= 2.0

    Any value under the weak threshold is not considered strong enough to make a case
    for El Nino or La Nina categorization.

    :param start_date: yyyy-mm-dd Formatted date to begin measurements.
    :param end_date: yyyy-mm-dd Formatted date to end measurements
    :param export_to_file: When true, export to a csv file with the dates as the name.
    :return: State object
    '''
    url = "https://origin.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/ONI_v5.php"
    open_url = urlopen(url)
    soup = BeautifulSoup(open_url.read(), features="lxml")

    table_data = []
    # Find the table that contains the rolling 3 month ENSO values
    for table in soup.find_all("table"):
        for tr in table.find_all("tr"):
            comments = tr.find_all(string=_is_comment)
            # This is a bit of a hack, but let's find the unofficial start to the tables
            # in the webpage. 
            if "Headings" in [x.string.strip() for x in comments]:
                for td in tr.find_all("td"):
                    table_data.append(td.text.strip())
    
                break  # the main table should have all we need

        if table_data:
            break  # as long as the table data isn't empty we should have what we need

    try:
        idx = table_data.index('Year')
        table_data = table_data[idx:]

        # The header is always going to be the same
        header = ['Year', 'DJF', 'JFM', 'FMA', 'MAM', 'AMJ', 'MJJ', 'JJA', 'JAS', 'ASO', 'SON', 'OND', 'NDJ']
        rows = []
        for data in range(0, len(table_data), len(header)):
            row = table_data[data:data+len(header)]
            if row != header:
                rows.append(row)


        df = pd.DataFrame(rows, columns=header)
        if export_to_file:
            df.to_csv("enso.csv", index=False)

        state = State(
            dataframe=df,
            exported_to_file=export_to_file,
            filename="enso.csv"
        )
        return state
            
    except ValueError:
        # failed to find the item, this is a problem
        return


def main():
    today = datetime.now()
    end_date = f"{today.year}-{today.month:02}-{today.day:02}"

    daily_data_state = get_daily_data(end_date=end_date, export_to_file=True)
    nao_state = get_nao_data(export_to_file=True)
    enso_state = get_enso_data(export_to_file=True)


if __name__ == '__main__':
    main()